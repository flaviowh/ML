{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tf_text\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import functools\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "DESKTOP = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')\n",
    "\n",
    "\n",
    "def kaggle_submission(predictions, outputname=None):\n",
    "    ids = test_df.id.astype(int).to_numpy()\n",
    "    if not outputname:\n",
    "        outputname= \"my_predictions\"\n",
    "    results = pd.DataFrame(zip(ids, predictions.round()), columns=[\"id\", \"target\"])\n",
    "    results.target = results.target.astype(int)\n",
    "    output = f\"{DESKTOP}/{outputname}.csv\"\n",
    "    results.to_csv(output, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Data for python\\\\Kaggle\\\\nlp-getting-started\\\\sample_submission.csv',\n",
       " 'D:\\\\Data for python\\\\Kaggle\\\\nlp-getting-started\\\\test.csv',\n",
       " 'D:\\\\Data for python\\\\Kaggle\\\\nlp-getting-started\\\\train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"D:\\Data for python\\Kaggle\\nlp-getting-started\"\n",
    "files = [os.path.join(PATH, file) for file in os.listdir(PATH) if file.endswith(\".csv\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(files[2])\n",
    "test_df = pd.read_csv(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = df.text.to_numpy()\n",
    "test_array = test_df.text.to_numpy()\n",
    "concat_text = ' '.join(text for text in text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Markets ablaze http://t.co/lHYXEOHY6C dsadas\"\n",
    "match = re.search(r\"http://t.co/.*($|\\s)\", sample)\n",
    "match.group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KERAS EMBEDDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "num_oov_buckets= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(text_array)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(x) for x in train_sequences])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
    "test_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_padded\n",
    "y = df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full , x_test , y_train_full, y_test =  train_test_split(x, y, test_size=0.1)\n",
    "x_valid, x_train =  x_train_full[6000:], x_train_full[:6000]\n",
    "y_valid, y_train = y_train_full[6000:],  y_train_full[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "vocab_size = num_words\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           input_shape=[None], mask_zero=True),\n",
    "    keras.layers.GRU(3,return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(3),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_accuracy\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, validation_data=[x_valid, y_valid], callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_true = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "metric = \"accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kaggle_predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD EMBEDDINGS WITH GLOVE AND NLTK CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 50\n",
    "glove_path = fr\"D:\\Data for python\\datasets\\glove.twitter.27B\\glove.twitter.27B.{EMBED_DIM}d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                embeddings[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = dict_from_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193515"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_token_list(text):\n",
    "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "  lemmatizer = nltk.WordNetLemmatizer()\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  lowercased_tokens = [token.lower() for token in tokens]\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]\n",
    "  useful_tokens = [token for token in lemmatized_tokens if token in embedding_dict]\n",
    "  return useful_tokens\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_words(train_data: np.array):\n",
    "  all_word_vector_sequences = []\n",
    "\n",
    "  for message in train_data[:,3]:\n",
    "    message_as_vector_seq = message_to_word_vectors(message)\n",
    "    if message_as_vector_seq.shape[0] == 0:\n",
    "      message_as_vector_seq = np.zeros(shape=(1, 50))\n",
    "\n",
    "    all_word_vector_sequences.append(message_as_vector_seq)\n",
    "  \n",
    "  return all_word_vector_sequences  \n",
    "\n",
    "\n",
    "\n",
    "def message_to_word_vectors(message, word_dict=embedding_dict):\n",
    "  processed_list_of_tokens = message_to_token_list(message)\n",
    "\n",
    "  vectors = []\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in word_dict:\n",
    "      continue\n",
    "    \n",
    "    token_vector = word_dict[token]\n",
    "    vectors.append(token_vector)\n",
    "  \n",
    "  return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full , x_test , y_train_full, y_test =  train_test_split(train_df.to_numpy(), y, test_size=0.1)\n",
    "x_valid, x_train =  x_train_full[5000:], x_train_full[:5000]\n",
    "y_valid, y_train = y_train_full[5000:],  y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 15 762 1851\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorize_words(x_train)\n",
    "x_test = vectorize_words(x_test)\n",
    "x_valid = vectorize_words(x_valid)\n",
    "\n",
    "\n",
    "print(len(X_train), len(X_train[0]), len(x_test), len(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  sequence_lengths.append(len(X_train[i]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean       15.317200\n",
       "std         5.763022\n",
       "min         1.000000\n",
       "25%        11.000000\n",
       "50%        15.000000\n",
       "75%        20.000000\n",
       "max        32.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sequence_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = pd.Series(sequence_lengths).max() + 2\n",
    "\n",
    "\n",
    "def pad_X(X, desired_sequence_length=MAX_SEQ_LEN):\n",
    "  padded_sequences = pad_sequences(X, maxlen=desired_sequence_length, padding='post', truncating='post')\n",
    "  return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 34, 50)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_X(X_train)\n",
    "x_test = pad_X(x_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 34, 50)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = pad_X(x_valid), y_valid\n",
    "X_val = pad_X(X_val)\n",
    "\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GUY'S MODEL\n",
    "\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.metrics import AUC\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# callback = ModelCheckpoint('model/', save_best_only=True)\n",
    "\n",
    "# model = keras.Sequential([])\n",
    "\n",
    "# model.add(keras.layers.Input(shape=(MAX_SEQ_LEN, 50)))\n",
    "# model.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer=\"Nadam\", \n",
    "#               loss=\"binary_crossentropy\", \n",
    "#               metrics=['accuracy', AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([])\n",
    "\n",
    "model.add(keras.layers.Input(shape=(MAX_SEQ_LEN, EMBED_DIM)))\n",
    "# model.add(keras.layers.GRU(3,return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(keras.layers.GRU(3))\n",
    "# model.add(keras.layers.Conv1D(filters=64, kernel_size=5,\n",
    "#           strides=2, padding=\"same\"))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "# model.add(keras.layers.Conv1D(filters=64, kernel_size=5,\n",
    "#               padding=\"same\"))\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(keras.layers.Bidirectional(keras.layers.GRU(16, return_sequences=True)))\n",
    "model.add(keras.layers.GRU(16, dropout=0.2,\n",
    "          recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(keras.layers.GRU(16, return_sequences=False))\n",
    "# model.add(keras.layers.GRU(128, dropout=0.2,\n",
    "#           recurrent_dropout=0.2, return_sequences=True))\n",
    "# model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_4 (Bidirectio  (None, 34, 32)           6528      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " gru_40 (GRU)                (None, 34, 16)            2400      \n",
      "                                                                 \n",
      " gru_41 (GRU)                (None, 16)                1632      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,577\n",
      "Trainable params: 10,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_accuracy\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "157/157 [==============================] - 22s 79ms/step - loss: 0.6765 - accuracy: 0.5816 - val_loss: 0.6605 - val_accuracy: 0.6180 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.5304 - accuracy: 0.7484 - val_loss: 0.4940 - val_accuracy: 0.7785 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4929 - accuracy: 0.7782 - val_loss: 0.4792 - val_accuracy: 0.7920 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4794 - accuracy: 0.7836 - val_loss: 0.4700 - val_accuracy: 0.7925 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.4721 - accuracy: 0.7916 - val_loss: 0.4624 - val_accuracy: 0.7952 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4663 - accuracy: 0.7926 - val_loss: 0.4624 - val_accuracy: 0.7979 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4572 - accuracy: 0.7928 - val_loss: 0.4686 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.4493 - accuracy: 0.7974 - val_loss: 0.4591 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.4414 - accuracy: 0.8006 - val_loss: 0.4751 - val_accuracy: 0.7904 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4358 - accuracy: 0.8044 - val_loss: 0.4688 - val_accuracy: 0.8012 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4304 - accuracy: 0.8056 - val_loss: 0.4770 - val_accuracy: 0.7893 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.4220 - accuracy: 0.8090 - val_loss: 0.4611 - val_accuracy: 0.7990 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "157/157 [==============================] - 15s 94ms/step - loss: 0.4159 - accuracy: 0.8122 - val_loss: 0.4647 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Epoch 13: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, callbacks=[callbacks]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCORE   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 3s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8162729658792651"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "accuracy_score(y_true, y_pred.round())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Best validation acc so far = 0.8241469816272966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 2s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "test_data = test_df.to_numpy()\n",
    "test_data = vectorize_words(test_data)\n",
    "test_data = pad_X(test_data)\n",
    "\n",
    "classifications = model.predict(test_data)\n",
    "\n",
    "submission = kaggle_submission(classifications, \"my_submission '-'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8a6ac2cf07d20788ee3be559b7c570527ae1980040eb9544e2a11bb3a26a390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
