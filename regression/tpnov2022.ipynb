{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import mrmr\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_regression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: combine thousands of prediction files to decrease the log error of a binary classification. Managed to decrease the error from 0.63 to ~ 0.529. Data is available at https://www.kaggle.com/competitions/tabular-playground-series-nov-2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(r\"D:\\Data for python\\datasets\\tabular-playground-series-nov-2022\\train_labels.csv\")\n",
    "submissions = [os.path.join(r'D:\\Data for python\\datasets\\tabular-playground-series-nov-2022\\submission_files', file) for file in os.listdir(r'D:\\Data for python\\datasets\\tabular-playground-series-nov-2022\\submission_files')]\n",
    "sample_sub = pd.read_csv(r\"D:\\Data for python\\datasets\\tabular-playground-series-nov-2022\\sample_submission.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_submission(trained_model, test_data, columns: list[str] = [\"id\", \"pred\"],savename=\"my_predictions\", neural=False, round=False):\n",
    "  SAVING_FOLDER = os.path.join(os.path.join(\n",
    "    os.environ['USERPROFILE']), 'Desktop')\n",
    "  predictions = trained_model.predict(test_data)\n",
    "  if neural:\n",
    "    predictions = [pred for [pred] in predictions]\n",
    "  if round:\n",
    "    predictions = [round(num) for num in predictions]\n",
    "  indices = range(20000, 40000)\n",
    "  sub_df = pd.DataFrame(zip(indices, predictions) , columns=columns)\n",
    "  with open(f\"{SAVING_FOLDER}/{savename}.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
    "    sub_df.to_csv(f, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(submissions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.709336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.675462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.481046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.957339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>39995</td>\n",
       "      <td>0.382515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>39996</td>\n",
       "      <td>0.352498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>39997</td>\n",
       "      <td>0.577554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>39998</td>\n",
       "      <td>0.712353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>39999</td>\n",
       "      <td>0.483215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      pred\n",
       "0          0  0.709336\n",
       "1          1  0.452988\n",
       "2          2  0.675462\n",
       "3          3  0.481046\n",
       "4          4  0.957339\n",
       "...      ...       ...\n",
       "39995  39995  0.382515\n",
       "39996  39996  0.352498\n",
       "39997  39997  0.577554\n",
       "39998  39998  0.712353\n",
       "39999  39999  0.483215\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBMISSIONS = 300\n",
    "\n",
    "df = df0.copy()\n",
    "\n",
    "for i, sub in enumerate(submissions[1:NUM_SUBMISSIONS]):\n",
    "    df = df.join(pd.read_csv(sub).pred, rsuffix=f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)\n",
    "x , test = df.iloc[:20000], df.iloc[20000:]\n",
    "y = labels.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRMR algorithm from https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs:\n",
    "#    X: pandas.DataFrame, features\n",
    "#    y: pandas.Series, target variable\n",
    "#    K: number of features to select\n",
    "X = x\n",
    "K = 100\n",
    "\n",
    "\n",
    "# compute F-statistics and initialize correlation matrix\n",
    "F = pd.Series(f_regression(X, y)[0], index = X.columns)\n",
    "corr = pd.DataFrame(.00001, index = X.columns, columns = X.columns)\n",
    "\n",
    "# initialize list of selected features and list of excluded features\n",
    "selected = []\n",
    "not_selected = X.columns.to_list()\n",
    "\n",
    "# repeat K times\n",
    "for i in range(K):\n",
    "  \n",
    "    # compute (absolute) correlations between the last selected feature and all the (currently) excluded features\n",
    "    if i > 0:\n",
    "        last_selected = selected[-1]\n",
    "        corr.loc[not_selected, last_selected] = X[not_selected].corrwith(X[last_selected]).abs().clip(.00001)\n",
    "        \n",
    "    # compute FCQ score for all the (currently) excluded features (this is Formula 2)\n",
    "    score = F.loc[not_selected] / corr.loc[not_selected, selected].mean(axis = 1).fillna(.00001)\n",
    "    \n",
    "    # find best feature, add it to selected and remove it from not_selected\n",
    "    best = score.index[score.argmax()]\n",
    "    selected.append(best)\n",
    "    not_selected.remove(best)\n",
    "\n",
    "\n",
    "x = x.drop(not_selected, axis=1)\n",
    "test = test.drop(not_selected, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_CUT = 16000\n",
    "\n",
    "x_train_full, x_test, y_train_full, y_test =  train_test_split(x, y, test_size=0.05)          \n",
    "x_train, x_valid =  x_train_full[:SPLIT_CUT], x_train_full[SPLIT_CUT:]   \n",
    "y_train, y_valid =  y_train_full[:SPLIT_CUT], y_train_full[SPLIT_CUT:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb = XGBClassifier(n_estimators= 200, learning_rate = 0.2)\n",
    "#forest = RandomForestClassifier(n_estimators=200)\n",
    "#xgb.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "#preds = xgb.predict(x_test.to_numpy())\n",
    "#log_loss(y_pred = preds, y_true=y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "500/500 [==============================] - 4s 6ms/step - loss: 0.2169 - val_loss: 0.1853\n",
      "Epoch 2/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1912 - val_loss: 0.1843\n",
      "Epoch 3/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1877 - val_loss: 0.1836\n",
      "Epoch 4/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1868 - val_loss: 0.1834\n",
      "Epoch 5/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1848 - val_loss: 0.1833\n",
      "Epoch 6/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.1839\n",
      "Epoch 7/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1838 - val_loss: 0.1835\n",
      "Epoch 8/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1832 - val_loss: 0.1849\n",
      "Epoch 9/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1829 - val_loss: 0.1846\n",
      "Epoch 10/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1829 - val_loss: 0.1832\n",
      "Epoch 11/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1820 - val_loss: 0.1833\n",
      "Epoch 12/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1819 - val_loss: 0.1831\n",
      "Epoch 13/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1817 - val_loss: 0.1839\n",
      "Epoch 14/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1814 - val_loss: 0.1838\n",
      "Epoch 15/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1816 - val_loss: 0.1835\n",
      "Epoch 16/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1815 - val_loss: 0.1832\n",
      "Epoch 17/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1816 - val_loss: 0.1831\n",
      "Epoch 18/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1811 - val_loss: 0.1837\n",
      "Epoch 19/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1810 - val_loss: 0.1835\n",
      "Epoch 20/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1811 - val_loss: 0.1837\n",
      "Epoch 21/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1815 - val_loss: 0.1835\n",
      "Epoch 22/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1807 - val_loss: 0.1839\n",
      "Epoch 23/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1808 - val_loss: 0.1832\n",
      "Epoch 24/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1805 - val_loss: 0.1826\n",
      "Epoch 25/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1804 - val_loss: 0.1834\n",
      "Epoch 26/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1802 - val_loss: 0.1850\n",
      "Epoch 27/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1804 - val_loss: 0.1837\n",
      "Epoch 28/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1803 - val_loss: 0.1838\n",
      "Epoch 29/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1800 - val_loss: 0.1830\n",
      "Epoch 30/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1801 - val_loss: 0.1843\n",
      "Epoch 31/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1805 - val_loss: 0.1841\n",
      "Epoch 32/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1800 - val_loss: 0.1842\n",
      "Epoch 33/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1800 - val_loss: 0.1836\n",
      "Epoch 34/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1798 - val_loss: 0.1837\n",
      "Epoch 35/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1796 - val_loss: 0.1828\n",
      "Epoch 36/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1797 - val_loss: 0.1826\n",
      "Epoch 37/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1797 - val_loss: 0.1833\n",
      "Epoch 38/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1795 - val_loss: 0.1832\n",
      "Epoch 39/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1791 - val_loss: 0.1840\n",
      "Epoch 40/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1795 - val_loss: 0.1838\n",
      "Epoch 41/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1791 - val_loss: 0.1837\n",
      "Epoch 42/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1793 - val_loss: 0.1841\n",
      "Epoch 43/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1789 - val_loss: 0.1828\n",
      "Epoch 44/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1788 - val_loss: 0.1852\n",
      "Epoch 45/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1792 - val_loss: 0.1828\n",
      "Epoch 46/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1791 - val_loss: 0.1836\n",
      "Epoch 47/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1792 - val_loss: 0.1831\n",
      "Epoch 48/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1789 - val_loss: 0.1850\n",
      "Epoch 49/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1790 - val_loss: 0.1836\n",
      "Epoch 50/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1789 - val_loss: 0.1840\n",
      "Epoch 51/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1788 - val_loss: 0.1823\n",
      "Epoch 52/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1784 - val_loss: 0.1841\n",
      "Epoch 53/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1786 - val_loss: 0.1819\n",
      "Epoch 54/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1791 - val_loss: 0.1834\n",
      "Epoch 55/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1788 - val_loss: 0.1837\n",
      "Epoch 56/200\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.1790 - val_loss: 0.1832\n",
      "Epoch 57/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1786 - val_loss: 0.1824\n",
      "Epoch 58/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1787 - val_loss: 0.1841\n",
      "Epoch 59/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1788 - val_loss: 0.1843\n",
      "Epoch 60/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1790 - val_loss: 0.1834\n",
      "Epoch 61/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1787 - val_loss: 0.1822\n",
      "Epoch 62/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1784 - val_loss: 0.1840\n",
      "Epoch 63/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1784 - val_loss: 0.1832\n",
      "Epoch 64/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1783 - val_loss: 0.1831\n",
      "Epoch 65/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1787 - val_loss: 0.1834\n",
      "Epoch 66/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1786 - val_loss: 0.1827\n",
      "Epoch 67/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1784 - val_loss: 0.1838\n",
      "Epoch 68/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1779 - val_loss: 0.1834\n",
      "Epoch 69/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1783 - val_loss: 0.1824\n",
      "Epoch 70/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1782 - val_loss: 0.1820\n",
      "Epoch 71/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1780 - val_loss: 0.1823\n",
      "Epoch 72/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1777 - val_loss: 0.1834\n",
      "Epoch 73/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1781 - val_loss: 0.1829\n",
      "Epoch 74/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1781 - val_loss: 0.1835\n",
      "Epoch 75/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1782 - val_loss: 0.1827\n",
      "Epoch 76/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1777 - val_loss: 0.1829\n",
      "Epoch 77/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1783 - val_loss: 0.1835\n",
      "Epoch 78/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1777 - val_loss: 0.1830\n",
      "Epoch 79/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1779 - val_loss: 0.1840\n",
      "Epoch 80/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1779 - val_loss: 0.1830\n",
      "Epoch 81/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1778 - val_loss: 0.1825\n",
      "Epoch 82/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1779 - val_loss: 0.1832\n",
      "Epoch 83/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1776 - val_loss: 0.1824\n",
      "Epoch 84/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1776 - val_loss: 0.1830\n",
      "Epoch 85/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1776 - val_loss: 0.1825\n",
      "Epoch 86/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1780 - val_loss: 0.1825\n",
      "Epoch 87/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1781 - val_loss: 0.1827\n",
      "Epoch 88/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1775 - val_loss: 0.1827\n",
      "Epoch 89/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1777 - val_loss: 0.1829\n",
      "Epoch 90/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1777 - val_loss: 0.1836\n",
      "Epoch 91/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1779 - val_loss: 0.1815\n",
      "Epoch 92/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1775 - val_loss: 0.1824\n",
      "Epoch 93/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1781 - val_loss: 0.1825\n",
      "Epoch 94/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1773 - val_loss: 0.1826\n",
      "Epoch 95/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1777 - val_loss: 0.1828\n",
      "Epoch 96/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1772 - val_loss: 0.1838\n",
      "Epoch 97/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1776 - val_loss: 0.1821\n",
      "Epoch 98/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1773 - val_loss: 0.1837\n",
      "Epoch 99/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1775 - val_loss: 0.1820\n",
      "Epoch 100/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1770 - val_loss: 0.1835\n",
      "Epoch 101/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1774 - val_loss: 0.1821\n",
      "Epoch 102/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1770 - val_loss: 0.1830\n",
      "Epoch 103/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1776 - val_loss: 0.1829\n",
      "Epoch 104/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1773 - val_loss: 0.1823\n",
      "Epoch 105/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1775 - val_loss: 0.1840\n",
      "Epoch 106/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1768 - val_loss: 0.1828\n",
      "Epoch 107/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1775 - val_loss: 0.1837\n",
      "Epoch 108/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1769 - val_loss: 0.1831\n",
      "Epoch 109/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1774 - val_loss: 0.1834\n",
      "Epoch 110/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1770 - val_loss: 0.1827\n",
      "Epoch 111/200\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1772 - val_loss: 0.1832\n",
      "Epoch 112/200\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1771 - val_loss: 0.1820\n",
      "Epoch 113/200\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 0.1771 - val_loss: 0.1826\n",
      "Epoch 114/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1776 - val_loss: 0.1828\n",
      "Epoch 115/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1772 - val_loss: 0.1831\n",
      "Epoch 116/200\n",
      "500/500 [==============================] - 2s 4ms/step - loss: 0.1769 - val_loss: 0.1820\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"loss\")\n",
    "\n",
    "sample_to_normalize = x_train\n",
    "norm_layer = keras.layers.Normalization(axis=1)\n",
    "norm_layer.adapt(tf.constant(sample_to_normalize))\n",
    "\n",
    "neural = keras.models.Sequential([\n",
    "norm_layer,\n",
    "keras.layers.Dense(64, activation=\"relu\",input_shape=x_train.shape[1:]),\n",
    "keras.layers.Dropout(rate=0.1),\n",
    "keras.layers.Dense(64,  \"relu\"),\n",
    "keras.layers.Dropout(rate=0.1),\n",
    "keras.layers.Dense(64,  \"relu\"),\n",
    "keras.layers.Dense(1, activation=None),\n",
    "])\n",
    "\n",
    "optimizer3 = keras.optimizers.SGD(learning_rate=0.003, nesterov=True) #keras.optimizers.RMSprop(lr=0.001) \n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "optimizer2 = keras.optimizers.SGD(learning_rate= 0.01)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "neural.compile(loss=\"mean_squared_error\", optimizer= optimizer3)\n",
    "\n",
    "history = neural.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=200, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kaggle_submission(neural, test, savename=\"300 to 100 mrmr col neural 64n\", neural = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n",
      "log loss: nan   accuracy score: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flavi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2442: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "c:\\Users\\flavi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2442: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "pred = neural.predict(x_test)\n",
    "y_true = y_test.to_numpy()\n",
    "y_pred = np.array([val[0] for val in pred])\n",
    "y_pred_rounded = [val[0].round() for val in pred]\n",
    "loss = metrics.log_loss( y_true , y_pred)\n",
    "acc_score = metrics.accuracy_score( y_true, y_pred_rounded)\n",
    "\n",
    "print(f\"log loss: {loss}   accuracy score: {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8a6ac2cf07d20788ee3be559b7c570527ae1980040eb9544e2a11bb3a26a390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
